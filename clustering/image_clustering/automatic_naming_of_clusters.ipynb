{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "import torch\n",
    "from transformers import BlipProcessor, BlipForConditionalGeneration\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from collections import defaultdict, Counter\n",
    "from typing import List\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# --- STEP 1: BLIP Captioning Setup ---\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "blip_processor = BlipProcessor.from_pretrained(\"Salesforce/blip-image-captioning-large\")\n",
    "blip_model = BlipForConditionalGeneration.from_pretrained(\"Salesforce/blip-image-captioning-large\").to(device)\n",
    "\n",
    "def generate_caption(image, text=None):\n",
    "    inputs = blip_processor(image, text, return_tensors=\"pt\").to(device)\n",
    "    out = blip_model.generate(**inputs, max_new_tokens=30)\n",
    "    return blip_processor.decode(out[0], skip_special_tokens=True)\n",
    "\n",
    "def generate_captions(\n",
    "    images: List[Image.Image],\n",
    "    batch_size: int = 16,\n",
    "    max_new_tokens: int = 50\n",
    ") -> List[str]:\n",
    "    \"\"\"\n",
    "    Generate captions for a list of PIL Images using BLIP, in batches.\n",
    "\n",
    "    Args:\n",
    "      images: list of PIL.Image objects (or paths you open to PIL).\n",
    "      batch_size: how many to process at once.\n",
    "      max_new_tokens: length of generated captions.\n",
    "\n",
    "    Returns:\n",
    "      List of caption strings, in the same order as `images`.\n",
    "    \"\"\"\n",
    "    all_captions = []\n",
    "    for i in range(0, len(images), batch_size):\n",
    "        batch_imgs = images[i : i + batch_size]\n",
    "        # Processor will resize, normalize, and pad for you\n",
    "        inputs = blip_processor(\n",
    "            images=batch_imgs,\n",
    "            return_tensors=\"pt\",\n",
    "            padding=\"max_length\",   # pad to longest in batch\n",
    "            truncation=True\n",
    "        ).to(device)\n",
    "\n",
    "        # generate captions for the batch\n",
    "        generated = blip_model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_new_tokens\n",
    "        )\n",
    "\n",
    "        # decode each sequence to text\n",
    "        batch_captions = [\n",
    "            blip_processor.decode(g, skip_special_tokens=True)\n",
    "            for g in generated\n",
    "        ]\n",
    "        all_captions.extend(batch_captions)\n",
    "\n",
    "    return all_captions\n",
    "\n",
    "# # --- Example usage ---\n",
    "# if __name__ == \"__main__\":\n",
    "#     # Suppose you have file paths:\n",
    "#     paths = [\n",
    "#                 'illustrations/deepclustering/Iterative.png', \n",
    "#                 'illustrations/deepclustering/Multi-Stage.png',\n",
    "#                 'illustrations/deepclustering/Simultaneous.png',\n",
    "#                 'illustrations/deepclustering/taxonomy.png',\n",
    "#                 'illustrations/deepclustering/generative.png'\n",
    "#              ]\n",
    "#     imgs = [Image.open(p).convert(\"RGB\") for p in paths]\n",
    "\n",
    "#     captions = generate_captions(imgs, batch_size=8)\n",
    "#     for img_path, cap in zip(paths, captions):\n",
    "#         print(f\"{img_path} â†’ {cap}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a diagram of a clusterer and a clusterer model',\n",
       " 'a diagram of a clusterer and clusterer module',\n",
       " 'a diagram of a clusterer and clusterer module',\n",
       " 'a diagram of a computer system with multiple levels of learning',\n",
       " 'a white and blue sign with the words clustering module and a pink and blue sign with the words cluster',\n",
       " 'araffes on a hill with a dog and a group of other animals',\n",
       " 'a diagram of a line graph with different colored dots',\n",
       " 'the big bang theory',\n",
       " 'araffes sitting on a couch with a group of friends eating']"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paths = [\n",
    "            'illustrations/deepclustering/Iterative.png', \n",
    "            'illustrations/deepclustering/Multi-Stage.png',\n",
    "            'illustrations/deepclustering/Simultaneous.png',\n",
    "            'illustrations/deepclustering/taxonomy.png',\n",
    "            'illustrations/deepclustering/generative.png',\n",
    "            '/Users/constantinseibold/workspace/CVPR_Tutorial/Identifying_Structure_In_Data/datasets/image_to_segment.png',\n",
    "            '/Users/constantinseibold/workspace/CVPR_Tutorial/Identifying_Structure_In_Data/output.png',\n",
    "            '/Users/constantinseibold/workspace/CVPR_Tutorial/Identifying_Structure_In_Data/clustering/image_clustering/71iPwWws0GL._AC_UF894,1000_QL80_.jpg',\n",
    "            '/Users/constantinseibold/workspace/CVPR_Tutorial/Identifying_Structure_In_Data/clustering/image_clustering/media.media.a99c5814-33e6-46c5-8f69-6fd0c8c9a162.16x9_1024.jpg'\n",
    "            ]\n",
    "imgs = [Image.open(p).convert(\"RGB\") for p in paths]\n",
    "\n",
    "captions = generate_captions(imgs, batch_size=8)\n",
    "captions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_clusters_by_captions(images, cluster_labels):\n",
    "    cluster_to_captions = defaultdict(list)\n",
    "\n",
    "    for img, cid in zip(images, cluster_labels):\n",
    "        if isinstance(img, str):\n",
    "            img = Image.open(img).convert(\"RGB\")\n",
    "        caption = generate_caption(img)\n",
    "        cluster_to_captions[cid].append(caption)\n",
    "\n",
    "    cluster_to_label = {}\n",
    "    for cid, caps in cluster_to_captions.items():\n",
    "        # Simple mode: most common noun/phrase\n",
    "        all_words = \" \".join(caps).lower().split()\n",
    "        most_common = Counter(all_words).most_common(3)\n",
    "        label = \", \".join([w for w, _ in most_common])\n",
    "        cluster_to_label[cid] = label\n",
    "\n",
    "    return cluster_to_label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Failed to import transformers.models.clip.modeling_clip because of the following error (look up to see its traceback):\ncannot import name 'auto_docstring' from 'transformers.utils' (/opt/anaconda3/envs/tutorial/lib/python3.11/site-packages/transformers/utils/__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mImportError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/tutorial/lib/python3.11/site-packages/transformers/utils/import_utils.py:1863\u001b[39m, in \u001b[36m_get_module\u001b[39m\u001b[34m(self, module_name)\u001b[39m\n\u001b[32m   1862\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(backend, Backend):\n\u001b[32m-> \u001b[39m\u001b[32m1863\u001b[39m     available, msg = backend.is_satisfied, backend.error_message\n\u001b[32m   1864\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/tutorial/lib/python3.11/importlib/__init__.py:126\u001b[39m, in \u001b[36mimport_module\u001b[39m\u001b[34m(name, package)\u001b[39m\n\u001b[32m    125\u001b[39m         level += \u001b[32m1\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m126\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_bootstrap\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_gcd_import\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m[\u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpackage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen importlib._bootstrap>:1204\u001b[39m, in \u001b[36m_gcd_import\u001b[39m\u001b[34m(name, package, level)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen importlib._bootstrap>:1176\u001b[39m, in \u001b[36m_find_and_load\u001b[39m\u001b[34m(name, import_)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen importlib._bootstrap>:1147\u001b[39m, in \u001b[36m_find_and_load_unlocked\u001b[39m\u001b[34m(name, import_)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen importlib._bootstrap>:690\u001b[39m, in \u001b[36m_load_unlocked\u001b[39m\u001b[34m(spec)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen importlib._bootstrap_external>:940\u001b[39m, in \u001b[36mexec_module\u001b[39m\u001b[34m(self, module)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen importlib._bootstrap>:241\u001b[39m, in \u001b[36m_call_with_frames_removed\u001b[39m\u001b[34m(f, *args, **kwds)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/tutorial/lib/python3.11/site-packages/transformers/models/clip/modeling_clip.py:28\u001b[39m\n\u001b[32m     27\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmodeling_utils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n\u001b[32m---> \u001b[39m\u001b[32m28\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ModelOutput, auto_docstring, can_return_tuple, logging, torch_int\n\u001b[32m     29\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mconfiguration_clip\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m CLIPConfig, CLIPTextConfig, CLIPVisionConfig\n",
      "\u001b[31mImportError\u001b[39m: cannot import name 'auto_docstring' from 'transformers.utils' (/opt/anaconda3/envs/tutorial/lib/python3.11/site-packages/transformers/utils/__init__.py)",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[82]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtransformers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m CLIPProcessor, CLIPModel\n\u001b[32m      3\u001b[39m clip_model = CLIPModel.from_pretrained(\u001b[33m\"\u001b[39m\u001b[33mopenai/clip-vit-base-patch32\u001b[39m\u001b[33m\"\u001b[39m).to(device)\n\u001b[32m      4\u001b[39m clip_processor = CLIPProcessor.from_pretrained(\u001b[33m\"\u001b[39m\u001b[33mopenai/clip-vit-base-patch32\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen importlib._bootstrap>:1229\u001b[39m, in \u001b[36m_handle_fromlist\u001b[39m\u001b[34m(module, fromlist, import_, recursive)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/tutorial/lib/python3.11/site-packages/transformers/utils/import_utils.py:1852\u001b[39m, in \u001b[36m__getattr__\u001b[39m\u001b[34m(self, name)\u001b[39m\n\u001b[32m   1848\u001b[39m     backends = [backends]\n\u001b[32m   1850\u001b[39m name = obj.__name__ if hasattr(obj, \"__name__\") else obj.__class__.__name__\n\u001b[32m-> \u001b[39m\u001b[32m1852\u001b[39m # Raise an error for users who might not realize that classes without \"TF\" are torch-only\n\u001b[32m   1853\u001b[39m if \"torch\" in backends and \"tf\" not in backends and not is_torch_available() and is_tf_available():\n\u001b[32m   1854\u001b[39m     raise ImportError(PYTORCH_IMPORT_ERROR_WITH_TF.format(name))\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/tutorial/lib/python3.11/site-packages/transformers/utils/import_utils.py:1851\u001b[39m, in \u001b[36m__getattr__\u001b[39m\u001b[34m(self, name)\u001b[39m\n\u001b[32m      0\u001b[39m <Error retrieving source code with stack_data see ipython/ipython#13598>\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/tutorial/lib/python3.11/site-packages/transformers/utils/import_utils.py:1865\u001b[39m, in \u001b[36m_get_module\u001b[39m\u001b[34m(self, module_name)\u001b[39m\n\u001b[32m   1863\u001b[39m     available, msg = backend.is_satisfied, backend.error_message\n\u001b[32m   1864\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1865\u001b[39m     available, msg = BACKENDS_MAPPING[backend]\n\u001b[32m   1867\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m available():\n\u001b[32m   1868\u001b[39m     failed.append(msg.format(name))\n",
      "\u001b[31mRuntimeError\u001b[39m: Failed to import transformers.models.clip.modeling_clip because of the following error (look up to see its traceback):\ncannot import name 'auto_docstring' from 'transformers.utils' (/opt/anaconda3/envs/tutorial/lib/python3.11/site-packages/transformers/utils/__init__.py)"
     ]
    }
   ],
   "source": [
    "from transformers import CLIPProcessor, CLIPModel\n",
    "\n",
    "clip_model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\").to(device)\n",
    "clip_processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "\n",
    "candidates = [\"dog\", \"cat\", \"car\", \"nature\", \"city\", \"food\", \"portrait\", \"art\", \"sports\"]\n",
    "\n",
    "def label_clusters_by_clip(images, cluster_labels, candidate_labels):\n",
    "    cluster_to_embeddings = defaultdict(list)\n",
    "\n",
    "    for img, cid in zip(images, cluster_labels):\n",
    "        if isinstance(img, str):\n",
    "            img = Image.open(img).convert(\"RGB\")\n",
    "        inputs = clip_processor(images=img, return_tensors=\"pt\").to(device)\n",
    "        outputs = clip_model.get_image_features(**inputs)\n",
    "        cluster_to_embeddings[cid].append(outputs.detach().cpu().numpy())\n",
    "\n",
    "    text_inputs = clip_processor(text=candidate_labels, return_tensors=\"pt\", padding=True).to(device)\n",
    "    text_embeds = clip_model.get_text_features(**text_inputs).detach().cpu().numpy()\n",
    "\n",
    "    cluster_to_label = {}\n",
    "    for cid, embeds in cluster_to_embeddings.items():\n",
    "        avg_embed = np.mean(embeds, axis=0)\n",
    "        sims = cosine_similarity([avg_embed], text_embeds)[0]\n",
    "        best_idx = int(np.argmax(sims))\n",
    "        cluster_to_label[cid] = candidate_labels[best_idx]\n",
    "\n",
    "    return cluster_to_label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from  openai import OpenAI\n",
    "\n",
    "def gpt_cluster_label(captions):\n",
    "    prompt = (\n",
    "        f\"Captions:\\n{chr(10).join(['- ' + c for c in captions])}\\n\\nLabel:\"\n",
    "    )\n",
    "\n",
    "    client = OpenAI()\n",
    "\n",
    "    response = client.chat.completions.create(\n",
    "            model=\"gpt-4o-mini\",\n",
    "            messages=[\n",
    "                {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": [\n",
    "                    {\n",
    "                    \"type\": \"text\",\n",
    "                    \"text\": \"You are given a list of captions generated from a group of similar images. \\\n",
    "                            Please summarize the common theme of these images in a short, descriptive label.\\n\\n\"\n",
    "                    }\n",
    "                ]\n",
    "                },{\n",
    "                \"role\": \"user\",\n",
    "                \"content\": [\n",
    "                    {\n",
    "                    \"type\": \"text\",\n",
    "                    \"text\": prompt\n",
    "                    }\n",
    "                ]\n",
    "                }\n",
    "            ]\n",
    "    )\n",
    "                    \n",
    "    # response = client.responses.create(\n",
    "    #     model=\"gpt-4.1\",\n",
    "    #     input=prompt\n",
    "    # )\n",
    "\n",
    "    print(response)\n",
    "\n",
    "    # response = openai.ChatCompletion.create(\n",
    "    #     model=\"gpt-4\",\n",
    "    #     messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "    #     temperature=0.3,\n",
    "    # )\n",
    "\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "def label_clusters_by_gpt(images, cluster_labels):\n",
    "    cluster_to_captions = defaultdict(list)\n",
    "\n",
    "    for img, cid in zip(images, cluster_labels):\n",
    "        if isinstance(img, str):\n",
    "            img = Image.open(img).convert(\"RGB\")\n",
    "        caption = generate_caption(img)\n",
    "        cluster_to_captions[cid].append(caption)\n",
    "\n",
    "    cluster_to_label = {}\n",
    "    for cid, caps in cluster_to_captions.items():\n",
    "        label = gpt_cluster_label(caps)\n",
    "        cluster_to_label[cid] = label\n",
    "\n",
    "    return cluster_to_label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Caption Labels: {0: 'a, clusterer, and', 1: 'a, araffes, on', 2: 'a, diagram, of', 3: 'a, the, big'}\n",
      "Imagenet Labels: {0: 'web site', 1: 'baboon', 2: 'envelope', 3: 'ice lolly'}\n",
      "GPT Labels: {0: '\"Clustering Models and Modules in Computer Systems\"', 1: '\"Animals Gathering on a Hill\"', 2: 'Data Visualization', 3: 'Friendship and Relaxation'}\n"
     ]
    }
   ],
   "source": [
    "images = [\n",
    "            'illustrations/deepclustering/Iterative.png', \n",
    "            'illustrations/deepclustering/Multi-Stage.png',\n",
    "            'illustrations/deepclustering/Simultaneous.png',\n",
    "            'illustrations/deepclustering/taxonomy.png',\n",
    "            'illustrations/deepclustering/generative.png',\n",
    "            '/Users/constantinseibold/workspace/CVPR_Tutorial/Identifying_Structure_In_Data/datasets/image_to_segment.png',\n",
    "            '/Users/constantinseibold/workspace/CVPR_Tutorial/Identifying_Structure_In_Data/output.png',\n",
    "            '/Users/constantinseibold/workspace/CVPR_Tutorial/Identifying_Structure_In_Data/clustering/image_clustering/71iPwWws0GL._AC_UF894,1000_QL80_.jpg',\n",
    "            '/Users/constantinseibold/workspace/CVPR_Tutorial/Identifying_Structure_In_Data/clustering/image_clustering/media.media.a99c5814-33e6-46c5-8f69-6fd0c8c9a162.16x9_1024.jpg'\n",
    "            ]\n",
    "\n",
    "# images = [\"/path/image1.jpg\", \"/path/image2.jpg\", ...]\n",
    "cluster_labels = [0, 0, 0,0,0, 1, 2, 3, 3]\n",
    "\n",
    "# caption_labels = label_clusters_by_captions(images, cluster_labels)\n",
    "\n",
    "imagenet_labels = label_clusters_by_imagenet(images, cluster_labels)\n",
    "\n",
    "# clip_labels = label_clusters_by_clip(images, cluster_labels, candidate_labels=[\n",
    "#     \"portrait\", \"landscape\", \"architecture\", \"animals\", \"vehicles\", \"food\", \"sports\", \"technology\"\n",
    "# ])\n",
    "\n",
    "# gpt_labels = label_clusters_by_gpt(images, cluster_labels)\n",
    "\n",
    "print(\"Caption Labels:\", caption_labels)\n",
    "print(\"Imagenet Labels:\", imagenet_labels)\n",
    "# print(\"CLIP Labels:\", clip_labels)\n",
    "print(\"GPT Labels:\", gpt_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clip_label_by_generated_captions(images, cluster_labels):\n",
    "    from sentence_transformers import SentenceTransformer\n",
    "    model = SentenceTransformer(\"clip-ViT-B-32\")\n",
    "\n",
    "    cluster_embeds = defaultdict(list)\n",
    "    cluster_captions = defaultdict(list)\n",
    "\n",
    "    # Step 1: Generate captions\n",
    "    for img, cid in zip(images, cluster_labels):\n",
    "        if isinstance(img, str): img = Image.open(img).convert(\"RGB\")\n",
    "        caption = generate_caption(img)\n",
    "        cluster_captions[cid].append(caption)\n",
    "        cluster_embeds[cid].append(model.encode(caption))\n",
    "\n",
    "    cluster_to_label = {}\n",
    "    for cid, cap_list in cluster_captions.items():\n",
    "        embeddings = model.encode(cap_list)\n",
    "        mean_embedding = np.mean(embeddings, axis=0)\n",
    "        sims = cosine_similarity([mean_embedding], embeddings)[0]\n",
    "        best_idx = np.argmax(sims)\n",
    "        cluster_to_label[cid] = cap_list[best_idx]  # Most central caption\n",
    "\n",
    "    return cluster_to_label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.transforms as T\n",
    "from torchvision.models.detection import fasterrcnn_resnet50_fpn\n",
    "from torchvision.ops import nms\n",
    "\n",
    "det_model = fasterrcnn_resnet50_fpn(pretrained=True).eval()\n",
    "transform = T.Compose([T.Resize((256, 256)), T.ToTensor()])\n",
    "\n",
    "COCO_LABELS = [\n",
    "    '__background__', 'person', 'bicycle', 'car', 'motorcycle', 'airplane', 'bus',\n",
    "    'train', 'truck', 'boat', 'traffic light', 'fire hydrant', 'stop sign', 'parking meter',\n",
    "    'bench', 'bird', 'cat', 'dog', 'horse', 'sheep', 'cow', 'elephant', 'bear', 'zebra',\n",
    "    'giraffe', 'backpack', 'umbrella', 'handbag', 'tie', 'suitcase', 'frisbee', 'skis',\n",
    "    'snowboard', 'sports ball', 'kite', 'baseball bat', 'baseball glove', 'skateboard',\n",
    "    'surfboard', 'tennis racket', 'bottle', 'wine glass', 'cup', 'fork', 'knife', 'spoon',\n",
    "    'bowl', 'banana', 'apple', 'sandwich', 'orange', 'broccoli', 'carrot', 'hot dog',\n",
    "    'pizza', 'donut', 'cake', 'chair', 'couch', 'potted plant', 'bed', 'dining table',\n",
    "    'toilet', 'tv', 'laptop', 'mouse', 'remote', 'keyboard', 'cell phone', 'microwave',\n",
    "    'oven', 'toaster', 'sink', 'refrigerator', 'book', 'clock', 'vase', 'scissors',\n",
    "    'teddy bear', 'hair drier', 'toothbrush'\n",
    "]\n",
    "\n",
    "def label_clusters_by_object_tags(images, cluster_labels, threshold=0.8):\n",
    "    cluster_objects = defaultdict(list)\n",
    "\n",
    "    for img, cid in zip(images, cluster_labels):\n",
    "        if isinstance(img, str): img = Image.open(img).convert(\"RGB\")\n",
    "        inp = transform(img).unsqueeze(0)\n",
    "        with torch.no_grad():\n",
    "            preds = det_model(inp)[0]\n",
    "\n",
    "        keep = preds[\"scores\"] > threshold\n",
    "        labels = preds[\"labels\"][keep].tolist()\n",
    "        label_names = [COCO_LABELS[i] for i in labels]\n",
    "        cluster_objects[cid].extend(label_names)\n",
    "\n",
    "    # Summarize by most frequent objects\n",
    "    return {\n",
    "        cid: \", \".join([w for w, _ in Counter(tags).most_common(3)])\n",
    "        for cid, tags in cluster_objects.items()\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "def label_clusters_with_lda_on_captions(images, cluster_labels, n_topics=1):\n",
    "    from nltk.corpus import stopwords\n",
    "    import nltk\n",
    "    nltk.download(\"stopwords\")\n",
    "\n",
    "    cluster_to_captions = defaultdict(list)\n",
    "    for img, cid in zip(images, cluster_labels):\n",
    "        if isinstance(img, str): img = Image.open(img).convert(\"RGB\")\n",
    "        cap = generate_caption(img)\n",
    "        cluster_to_captions[cid].append(cap)\n",
    "\n",
    "    cluster_to_label = {}\n",
    "    stop_words = stopwords.words(\"english\")\n",
    "\n",
    "    for cid, caps in cluster_to_captions.items():\n",
    "        vec = CountVectorizer(stop_words=stop_words, max_features=500)\n",
    "        X = vec.fit_transform(caps)\n",
    "        lda = LatentDirichletAllocation(n_components=n_topics).fit(X)\n",
    "        words = vec.get_feature_names_out()\n",
    "        topics = lda.components_\n",
    "        topic_words = [words[i] for i in topics[0].argsort()[-3:][::-1]]\n",
    "        cluster_to_label[cid] = \", \".join(topic_words)\n",
    "\n",
    "    return cluster_to_label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/tutorial/lib/python3.11/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/tutorial/lib/python3.11/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "from torchvision import models\n",
    "from torchvision.transforms import Compose, Resize, ToTensor, Normalize\n",
    "import torchvision.transforms as T\n",
    "\n",
    "resnet = models.resnet50(pretrained=True).eval()\n",
    "imagenet_labels = {i: c.strip() for i, c in enumerate(open(\"imagenet_classes.txt\"))}\n",
    "\n",
    "preprocess = Compose([\n",
    "    Resize(256),\n",
    "    T.CenterCrop(224),\n",
    "    ToTensor(),\n",
    "    Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "def label_clusters_by_imagenet(images, cluster_labels):\n",
    "    cluster_preds = defaultdict(list)\n",
    "\n",
    "    for img, cid in zip(images, cluster_labels):\n",
    "        if isinstance(img, str): img = Image.open(img).convert(\"RGB\")\n",
    "        inp = preprocess(img).unsqueeze(0)\n",
    "        with torch.no_grad():\n",
    "            logits = resnet(inp)\n",
    "        pred = logits.argmax(dim=1).item()\n",
    "        cluster_preds[cid].append(imagenet_labels[pred])\n",
    "\n",
    "    return {\n",
    "        cid: Counter(preds).most_common(1)[0][0]\n",
    "        for cid, preds in cluster_preds.items()\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "def label_clusters_ctfidf(images, cluster_labels, generate_caption, top_n=3):\n",
    "    \"\"\"\n",
    "    images: list of image paths or PIL Images\n",
    "    cluster_labels: list of cluster IDs (same length as images)\n",
    "    generate_caption: fn(image) -> str\n",
    "    top_n: number of terms to return per cluster\n",
    "    \"\"\"\n",
    "    # 1. Build pseudo-documents: {cluster_id: concatenated captions}\n",
    "    docs = defaultdict(list)\n",
    "    for img, cid in zip(images, cluster_labels):\n",
    "        caption = generate_caption(img)\n",
    "        docs[cid].append(caption.lower())\n",
    "\n",
    "    # Now docs_strs is list of strings, one per cluster\n",
    "    cluster_ids = list(docs.keys())\n",
    "    docs_strs   = [\" \".join(docs[cid]) for cid in cluster_ids]\n",
    "\n",
    "    # 2. Compute TF-IDF where each document = one cluster\n",
    "    #    min_df=1 to include rare words; you can tune stop_words, ngram_range, etc.\n",
    "    vectorizer = TfidfVectorizer(\n",
    "        stop_words=\"english\",\n",
    "        min_df=1,\n",
    "        ngram_range=(1,2),   # unigrams + bigrams often help\n",
    "    )\n",
    "    X = vectorizer.fit_transform(docs_strs)   # shape: (n_clusters, n_terms)\n",
    "    terms = vectorizer.get_feature_names_out()\n",
    "\n",
    "    # 3. Extract top_n terms per cluster by highest TF-IDF score\n",
    "    cluster_to_labels = {}\n",
    "    for idx, cid in enumerate(cluster_ids):\n",
    "        row = X[idx].toarray().flatten()\n",
    "        top_indices = row.argsort()[::-1][:top_n]\n",
    "        top_terms   = [terms[i] for i in top_indices]\n",
    "        cluster_to_labels[cid] = top_terms\n",
    "\n",
    "    return cluster_to_labels\n",
    "\n",
    "# --- Example usage ---\n",
    "if __name__ == \"__main__\":\n",
    "    # dummy stubs:\n",
    "    def generate_caption(img):\n",
    "        # replace this with your BLIP/mPLUG/etc. captioner\n",
    "        return \"a photo of a cat and a dog\"\n",
    "\n",
    "    images = [\"img1.jpg\", \"img2.jpg\", \"img3.jpg\", \"img4.jpg\"]\n",
    "    cluster_labels = [0, 0, 1, 1]\n",
    "\n",
    "    labels = label_clusters_ctfidf(images, cluster_labels, generate_caption, top_n=3)\n",
    "    print(\"Cluster labels:\", labels)\n",
    "    # e.g. Cluster labels: {0: [\"cat\", \"dog\", \"photo\"], 1: [\"outdoor\", \"tree\", \"sky\"]}\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tutorial",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
