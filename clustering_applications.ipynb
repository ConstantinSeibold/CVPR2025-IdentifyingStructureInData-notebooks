{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision.datasets import Imagenette\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn.functional as F\n",
    "from urllib.request import urlopen\n",
    "from PIL import Image\n",
    "from open_clip import create_model_from_pretrained, get_tokenizer\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## set dataset\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)), \n",
    "    transforms.ToTensor()         \n",
    "])\n",
    "\n",
    "dataset = Imagenette(root='./imagenette2-320', split='train', size='320px',transform=transform, download=True)\n",
    "dataloader = DataLoader(dataset, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## set model \n",
    "\n",
    "device = 'mps'\n",
    "model, preprocess = create_model_from_pretrained('hf-hub:timm/ViT-B-16-SigLIP2')\n",
    "model = model.to(device)\n",
    "tokenizer = get_tokenizer('hf-hub:timm/ViT-B-16-SigLIP2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/dw/_7yqpqlx54d_ymd52tfx0ygc0000gn/T/ipykernel_51615/701537802.py:6: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.no_grad(), torch.cuda.amp.autocast():\n",
      "/opt/anaconda3/envs/tutorial/lib/python3.11/site-packages/torch/amp/autocast_mode.py:266: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
      "  warnings.warn(\n",
      "  0%|          | 0/148 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 148/148 [01:06<00:00,  2.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "feature shape (9469, 768)\n",
      "labels shape (9469,)\n"
     ]
    }
   ],
   "source": [
    "## Extract Features \n",
    "\n",
    "features = []\n",
    "labels = []\n",
    "\n",
    "with torch.no_grad(), torch.cuda.amp.autocast():\n",
    "    for i, (images, label) in enumerate(tqdm(dataloader)):\n",
    "        features += [model.forward(images.to(device))[0]]\n",
    "        labels += [label]\n",
    "        \n",
    "features = torch.concat(features).cpu().numpy()\n",
    "labels = torch.concat(labels).cpu().numpy()\n",
    "\n",
    "\n",
    "print('feature shape',features.shape)\n",
    "print('labels shape',labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/tutorial/lib/python3.11/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/tutorial/lib/python3.11/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Partition 0: 1230 clusters\n",
      "Partition 1: 142 clusters\n",
      "Partition 2: 26 clusters\n",
      "Partition 3: 10 clusters\n",
      "Partition 4: 2 clusters\n",
      "/n ########### /n \n",
      "HDBSCAN Labels: [-1  0  1  2  3  4  5  6  7  8  9 10]\n",
      "DBSCAN Labels: [-1  0  1]\n",
      "KMeans Labels: [0 1 2 3 4 5 6 7 8 9]\n",
      "KMeans++ Labels: [0 1 2 3 4 5 6 7 8 9]\n",
      "FINCH Labels: [0 1 2 3 4 5 6 7 8 9]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import hdbscan\n",
    "from sklearn.cluster import DBSCAN, KMeans\n",
    "from sklearn.metrics import pairwise_distances\n",
    "from finch import FINCH\n",
    "from time import time\n",
    "\n",
    "start_time = time()\n",
    "# HDBSCAN clustering\n",
    "hdbscan_clusterer = hdbscan.HDBSCAN(min_cluster_size=10)\n",
    "hdbscan_labels = hdbscan_clusterer.fit_predict(features)\n",
    "end_time = time()\n",
    "hdbscan_time = end_time - start_time\n",
    "\n",
    "# DBSCAN clustering\n",
    "start_time = time()\n",
    "dbscan_clusterer = DBSCAN(eps=0.3, min_samples=10)\n",
    "dbscan_labels = dbscan_clusterer.fit_predict(features)\n",
    "end_time = time()\n",
    "dbscan_time = end_time - start_time\n",
    "\n",
    "# KMeans clustering\n",
    "start_time = time()\n",
    "kmeans_clusterer = KMeans(n_clusters=10, random_state=42)\n",
    "kmeans_labels = kmeans_clusterer.fit_predict(features)\n",
    "end_time = time()\n",
    "kmeans_time = end_time - start_time\n",
    "\n",
    "# KMeans++ clustering (KMeans with k-means++ initialization)\n",
    "start_time = time()\n",
    "kmeans_pp_clusterer = KMeans(n_clusters=10, init='k-means++', random_state=42)\n",
    "kmeans_pp_labels = kmeans_pp_clusterer.fit_predict(features)\n",
    "end_time = time()\n",
    "kmeans_pp_time = end_time - start_time\n",
    "\n",
    "# FINCH clustering\n",
    "start_time = time()\n",
    "c, num_clusters, _ = FINCH(features)\n",
    "finch_labels = c[:, -2]  # Get the final clustering result\n",
    "end_time = time()\n",
    "finch_time = end_time - start_time\n",
    "\n",
    "times = {\n",
    "    'HDBSCAN': hdbscan_time,\n",
    "    'DBSCAN': dbscan_time,\n",
    "    'KMeans': kmeans_time,\n",
    "    'KMeans++': kmeans_pp_time,\n",
    "    'FINCH': finch_time,\n",
    "}\n",
    "\n",
    "print('/n ########### /n ')\n",
    "\n",
    "print(\"HDBSCAN Labels:\", np.unique(hdbscan_labels))\n",
    "print(\"DBSCAN Labels:\", np.unique(dbscan_labels))\n",
    "print(\"KMeans Labels:\", np.unique(kmeans_labels))\n",
    "print(\"KMeans++ Labels:\", np.unique(kmeans_pp_labels))\n",
    "print(\"FINCH Labels:\", np.unique(finch_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HDBSCAN ->         Time: 66.6139,         ARI: 0.9983,         NMI: 0.9980,         FMI: 0.9984,         Silhouette: 0.2549\n",
      "DBSCAN ->         Time: 0.3103,         ARI: 0.0000,         NMI: 0.0000,         FMI: 0.7051,         Silhouette: 0.1419\n",
      "KMeans ->         Time: 0.1533,         ARI: 0.8544,         NMI: 0.9390,         FMI: 0.8726,         Silhouette: 0.1768\n",
      "KMeans++ ->         Time: 0.1475,         ARI: 0.8544,         NMI: 0.9390,         FMI: 0.8726,         Silhouette: 0.1768\n",
      "FINCH ->         Time: 0.3468,         ARI: 0.8624,         NMI: 0.9412,         FMI: 0.8796,         Silhouette: 0.1735\n"
     ]
    }
   ],
   "source": [
    "# compare clusterings\n",
    "from sklearn.metrics import adjusted_rand_score, normalized_mutual_info_score, fowlkes_mallows_score, silhouette_score\n",
    "\n",
    "def evaluate_clustering(labels, gt_labels, features):\n",
    "    valid_mask = labels != -1  # Ignore noise points (-1) for DBSCAN/HDBSCAN\n",
    "    if np.any(valid_mask):\n",
    "        labels = labels[valid_mask]\n",
    "        gt_labels = gt_labels[valid_mask]\n",
    "        features = features[valid_mask]\n",
    "\n",
    "    ari = adjusted_rand_score(gt_labels, labels)\n",
    "    nmi = normalized_mutual_info_score(gt_labels, labels)\n",
    "    fmi = fowlkes_mallows_score(gt_labels, labels)\n",
    "    silhouette = silhouette_score(features, labels) if len(set(labels)) > 1 else -1  # Silhouette needs >1 cluster\n",
    "\n",
    "    return {\"ARI\": ari, \"NMI\": nmi, \"FMI\": fmi, \"Silhouette\": silhouette}\n",
    "\n",
    "# Evaluate all clusterings\n",
    "results = {\n",
    "    \"HDBSCAN\": evaluate_clustering(hdbscan_labels, labels, features),\n",
    "    \"DBSCAN\": evaluate_clustering(dbscan_labels, labels, features),\n",
    "    \"KMeans\": evaluate_clustering(kmeans_labels, labels, features),\n",
    "    \"KMeans++\": evaluate_clustering(kmeans_pp_labels, labels, features),\n",
    "    \"FINCH\": evaluate_clustering(finch_labels, labels, features),\n",
    "}\n",
    "\n",
    "# Print results\n",
    "for method, scores in results.items():\n",
    "    print(f\"{method} -> \\\n",
    "        Time: {times[method]:.4f}, \\\n",
    "        ARI: {scores['ARI']:.4f}, \\\n",
    "        NMI: {scores['NMI']:.4f}, \\\n",
    "        FMI: {scores['FMI']:.4f}, \\\n",
    "        Silhouette: {scores['Silhouette']:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".conda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
